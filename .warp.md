# LinkedIn Ingestion Project Rules

## üß† CRITICAL MEMORY SYSTEMS - READ FIRST
**Essential files to check for previous learnings and context:**
1. **`MEMORY_KEEPER_MCP_GUIDE.md`** - Complete MCP usage with correct parameter formats  
2. **`learning/lessons-learned.md`** - Token waste prevention and critical learnings
3. **Memory Keeper MCP** - Query with `search_nodes` for `LinkedIn Ingestion` or `Supabase_CLI`
4. **`.warp.md`** (this file) - Project-specific patterns and connection details

When working on this project:

## Project Context
- This is a FastAPI project for LinkedIn data ingestion and processing
- The codebase has been recently consolidated and cleaned up (session 2025-08-07)
- All API endpoints are consolidated in `main.py` 
- All 167 tests are consolidated in `app/tests/` directory
- Use `python run_tests.py` to run the test suite
- Integrated with Supabase for database and API functionality

## Development Workflow
- Check `linkedin-ingestion-SESSION_HISTORY.md` for recent work context
- Session files are stored in `sessions/` directory
- Current work: V1.88 Prompt Templates Management System implementation
- Memory Keeper MCP should be used for all context persistence
- Hybrid AgentOS + WARP.md approach for robust session management

## Memory Management
- Use Memory Keeper MCP to store key entities and relationships
- V1.85 LLM Profile Scoring: COMPLETE (August 12th, 2025)
- Current focus: V1.88 Prompt Templates Management System
- Session hibernation should create timestamped files in sessions/ directory
- Always update SESSION_HISTORY.md for continuity between sessions
- Use Warp Terminal's built-in context through this .warp.md file

## Architecture Notes
- Removed deprecated API route files from `app/api/routes/*.py`
- Eliminated scattered test files from root directory
- Unified test execution with single runner script
- V1.85 LLM-based profile scoring: COMPLETE with OpenAI integration
- V1.88 Prompt templates system: Database-driven template management
- Production-first approach based on V1.85 lessons learned

## Testing
- All tests pass after structural consolidation
- Use unified test runner: `python run_tests.py`
- Test directory: `app/tests/`
- Total test count: 247 tests (with complete V1.85 implementation)
- Always test with virtual environment: `source venv/bin/activate`

## V1.85 LLM Profile Scoring Status: ‚úÖ COMPLETE
- ‚úÖ Task 1: Database Schema & Job Infrastructure (COMPLETE)
- ‚úÖ Task 2: OpenAI Integration & LLM Service (COMPLETE)
- ‚úÖ Task 3: API Endpoints Implementation (COMPLETE)
- ‚úÖ Task 4: Async Job Processing System (COMPLETE)
- ‚úÖ Task 5: Integration Testing & Production Deployment (COMPLETE)
- Production validated with Christopher Leslie profile scoring

## V1.88 Prompt Templates Management Status: üöß READY FOR IMPLEMENTATION
- ‚ùå Task 1: Database Schema & Migration Implementation
- ‚ùå Task 2: Pydantic Models & Data Validation
- ‚ùå Task 3: Template Service Layer Implementation
- ‚ùå Task 4: API Endpoints & Controllers
- ‚ùå Task 5: LLM Scoring Integration
- ‚ùå Task 6: Comprehensive Testing Implementation
- ‚ùå Task 7: Health Check & Monitoring Integration

## Key Context Files
- `agent-os/specs/2025-08-13-v188-prompt-templates-management/` - Current V1.88 spec for implementation
- `agent-os/specs/2025-08-11-v185-llm-profile-scoring/` - COMPLETED V1.85 LLM scoring system
- `agent-os/product/roadmap.md` - Project roadmap and current status
- `sessions/linkedin-ingestion-session-2025-08-12-225847.md` - V1.85 completion and V1.88 planning

## Production Environment
- **Domain**: smooth-mailbox-production.up.railway.app
- **API Key**: li_HieZz-IjBp0uE7d-rZkRE0qyy12r5_ZJS_FR4jMvv0I
- **Database**: Supabase with scoring_jobs table deployed
- **LLM Scoring**: Fully operational with OpenAI integration
- **Test Profile ID**: 435ccbf7-6c5e-4e2d-bdc3-052a244d7121 (Christopher Leslie)

## üö® CRITICAL: Supabase CLI Operations - PREVENT TOKEN WASTE

**MEMORIZE: linkedin-ingestion Project Database Details**
- **Service Role Password**: `dvm2rjq6ngk@GZN-wth` (CLI operations only)
- **Project URL**: `https://nggbqpkfdbhakbkbtxgc.supabase.co`
- **Project ID**: `nggbqpkfdbhakbkbtxgc`

**MANDATORY Command Templates (ALWAYS include password):**
```bash
# Database dump - NEVER experiment, use this exact format
supabase db dump -p "dvm2rjq6ngk@GZN-wth" -s public

# Migration push - NEVER omit password
supabase db push -p "dvm2rjq6ngk@GZN-wth"

# Migration repair - ALWAYS include password
supabase migration repair [timestamp] -p "dvm2rjq6ngk@GZN-wth"
```

**TOKEN WASTE PREVENTION RULES:**
1. NEVER try CLI commands without `-p "dvm2rjq6ngk@GZN-wth"`
2. NEVER use API/SQL editor when user says "use CLI"
3. NEVER search for these connection details - they're documented here
4. ALWAYS start with proven templates, never experiment
5. Service role password for CLI, regular password for direct connections
