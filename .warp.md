# LinkedIn Ingestion Project Rules

## üß† CRITICAL MEMORY SYSTEMS - READ FIRST
**Essential files to check for previous learnings and context:**
1. **`MEMORY_KEEPER_MCP_GUIDE.md`** - Complete MCP usage with correct parameter formats  
2. **`learning/lessons-learned.md`** - Token waste prevention and critical learnings
3. **Memory Keeper MCP** - Query with `search_nodes` for `LinkedIn Ingestion` or `Supabase_CLI`
4. **`.warp.md`** (this file) - Project-specific patterns and connection details

When working on this project:

## Project Context
- This is a FastAPI project for LinkedIn data ingestion and processing
- The codebase has been recently consolidated and cleaned up (session 2025-08-07)
- All API endpoints are consolidated in `main.py` 
- All 167 tests are consolidated in `app/tests/` directory
- Use `python run_tests.py` to run the test suite
- Integrated with Supabase for database and API functionality

## Development Workflow
- Check `linkedin-ingestion-SESSION_HISTORY.md` for recent work context
- Session files are stored in `sessions/` directory
- Current work: V1.88 Prompt Templates Management System implementation
- Memory Keeper MCP should be used for all context persistence
- Hybrid AgentOS + WARP.md approach for robust session management

## Memory Management
- Use Memory Keeper MCP to store key entities and relationships
- V1.85 LLM Profile Scoring: COMPLETE (August 12th, 2025)
- Current focus: V1.88 Prompt Templates Management System
- Session hibernation should create timestamped files in sessions/ directory
- Always update SESSION_HISTORY.md for continuity between sessions
- Use Warp Terminal's built-in context through this .warp.md file

## Architecture Notes
- Removed deprecated API route files from `app/api/routes/*.py`
- Eliminated scattered test files from root directory
- Unified test execution with single runner script
- V1.85 LLM-based profile scoring: COMPLETE with OpenAI integration
- V1.88 Prompt templates system: Database-driven template management
- Production-first approach based on V1.85 lessons learned

## Testing
- All tests pass after structural consolidation
- Use unified test runner: `python run_tests.py`
- Test directory: `app/tests/`
- Total test count: 247 tests (with complete V1.85 implementation)
- Always test with virtual environment: `source venv/bin/activate`

## V1.85 LLM Profile Scoring Status: ‚úÖ COMPLETE
- ‚úÖ Task 1: Database Schema & Job Infrastructure (COMPLETE)
- ‚úÖ Task 2: OpenAI Integration & LLM Service (COMPLETE)
- ‚úÖ Task 3: API Endpoints Implementation (COMPLETE)
- ‚úÖ Task 4: Async Job Processing System (COMPLETE)
- ‚úÖ Task 5: Integration Testing & Production Deployment (COMPLETE)
- Production validated with Christopher Leslie profile scoring

## V1.88 Prompt Templates Management Status: üöß READY FOR IMPLEMENTATION
- ‚ùå Task 1: Database Schema & Migration Implementation
- ‚ùå Task 2: Pydantic Models & Data Validation
- ‚ùå Task 3: Template Service Layer Implementation
- ‚ùå Task 4: API Endpoints & Controllers
- ‚ùå Task 5: LLM Scoring Integration
- ‚ùå Task 6: Comprehensive Testing Implementation
- ‚ùå Task 7: Health Check & Monitoring Integration

## Key Context Files
- `agent-os/specs/2025-08-13-v188-prompt-templates-management/` - Current V1.88 spec for implementation
- `agent-os/specs/2025-08-11-v185-llm-profile-scoring/` - COMPLETED V1.85 LLM scoring system
- `agent-os/product/roadmap.md` - Project roadmap and current status
- `sessions/linkedin-ingestion-session-2025-08-12-225847.md` - V1.85 completion and V1.88 planning

## Production Environment
- **Domain**: smooth-mailbox-production.up.railway.app
- **API Key**: li_HieZz-IjBp0uE7d-rZkRE0qyy12r5_ZJS_FR4jMvv0I
- **Database**: Supabase with scoring_jobs table deployed
- **LLM Scoring**: Fully operational with OpenAI integration
- **Test Profile ID**: 435ccbf7-6c5e-4e2d-bdc3-052a244d7121 (Christopher Leslie)

## üö® CRITICAL: Railway Logs - TERMINAL-SAFE API SYSTEM ‚úÖ FIXED

**NEW: Advanced Railway Logs System - NO MORE TERMINAL HANGS!**

**SAFE RAILWAY LOGS COMMANDS (GraphQL API-based):**
1. **Basic Usage:**
   ```bash
   ./safe_logs.sh                           # Default: deployment logs, last hour
   ./safe_logs.sh deployment --limit 50     # 50 deployment log entries
   ./safe_logs.sh build --limit 20          # Build logs
   ./safe_logs.sh deployment --errors-only  # Only errors and warnings
   ```

2. **Pre-built Debugging Patterns:**
   ```bash
   ./safe_logs.sh examples errors_last_hour    # All errors from last hour
   ./safe_logs.sh examples http_5xx            # HTTP 500-level errors 
   ./safe_logs.sh examples database_errors     # Database connection errors
   ./safe_logs.sh examples slow_requests       # Performance issues
   ./safe_logs.sh examples memory_warnings     # Memory usage warnings
   ./safe_logs.sh examples recent_builds       # Recent build activity
   ```

3. **Advanced Filtering:**
   ```bash
   ./safe_logs.sh deployment --json            # JSON output for analysis
   ./safe_logs.sh deployment --last-minutes 30 # Last 30 minutes only
   ./safe_logs.sh deployment --filter "@level:error OR timeout"
   ./safe_logs.sh http --status-code 500       # HTTP errors specifically
   ```

4. **Shell Aliases (configured in ~/.zshrc):**
   ```bash
   rlogs                 # Quick access: ./safe_logs.sh
   rlogs-errors          # Quick errors: ./safe_logs.sh examples errors_last_hour
   ```

5. **Railway web dashboard** (for comprehensive analysis):
   ```bash
   railway open   # Opens project dashboard in browser
   ```

**SYSTEM FEATURES:**
- ‚úÖ **Never hangs** - Uses Railway GraphQL API with bounded queries
- ‚úÖ **All log types** - Deployment, build, HTTP logs with advanced filtering
- ‚úÖ **Color-coded output** - Severity levels with visual indicators
- ‚úÖ **JSON support** - For programmatic processing and monitoring
- ‚úÖ **Rate limiting** - Built-in API rate limiting and retry logic
- ‚úÖ **Time ranges** - Precise control over log time windows
- ‚úÖ **Pre-built patterns** - Common debugging scenarios ready-to-use

**MIGRATION COMPLETE:**
- ‚ùå OLD: `railway logs` ‚Üí ‚úÖ NEW: `./safe_logs.sh`
- ‚ùå OLD: Terminal hangs ‚Üí ‚úÖ NEW: Clean exits every time
- ‚ùå OLD: Limited filtering ‚Üí ‚úÖ NEW: Full Railway filter syntax
- ‚ùå OLD: CLI timeout issues ‚Üí ‚úÖ NEW: GraphQL API reliability

## üîç RAILWAY WEBHOOK DEBUGGING - ISSUE IDENTIFIED

**ROOT CAUSE: Railway cannot reach localhost webhook URLs from their servers**

**Webhook Detection Logic:** ‚úÖ WORKING CORRECTLY
- Tests confirmed webhook detection patterns work properly
- Successfully detects: `deployment.success`, `status: 'SUCCESS'`, `status: 'DEPLOYED'` 
- Correctly ignores: `deployment.started`, `deployment.building`, `deployment.failed`

**Core Problem:** ‚ùå NETWORK ACCESSIBILITY
- Railway servers cannot reach `http://localhost:3000` from external network
- Webhook listener works locally but Railway can't send webhooks to localhost
- Previous webhook setup attempts failed due to this fundamental issue

**IMPROVED DEPLOYMENT MONITORING SOLUTIONS:**
1. **Smart Deployment Monitor** (recommended):
   ```bash
   python3 scripts/smart_deployment_monitor.py --timeout 600 --interval 15
   ```
   - Uses health endpoint polling (no network issues)
   - Detects service restarts and version changes
   - Monitors enhanced endpoint availability
   - Much more reliable than webhook approach

2. **Webhook with ngrok tunnel** (advanced):
   ```bash
   python3 scripts/smart_deployment_monitor.py --use-webhook
   ```
   - Creates public tunnel with ngrok (if installed)
   - Provides setup instructions for Railway webhook configuration
   - Falls back to polling if tunnel fails

**CRITICAL LESSON:** Never rely on localhost webhooks for external services
- Always use polling-based monitoring for Railway deployments
- Webhooks require public endpoints or tunneling solutions
- Health endpoint polling is more reliable and simpler to implement

## üö® CRITICAL: Supabase CLI Operations - PREVENT TOKEN WASTE

**MEMORIZE: linkedin-ingestion Project Database Details**
- **Service Role Password**: `dvm2rjq6ngk@GZN-wth` (CLI operations only)
- **Project URL**: `https://nggbqpkfdbhakbkbtxgc.supabase.co`
- **Project ID**: `nggbqpkfdbhakbkbtxgc`

**MANDATORY Command Templates (ALWAYS include password):**
```bash
# Database dump - NEVER experiment, use this exact format
supabase db dump -p "dvm2rjq6ngk@GZN-wth" -s public

# Migration push - NEVER omit password
supabase db push -p "dvm2rjq6ngk@GZN-wth"

# Migration repair - ALWAYS include password
supabase migration repair [timestamp] -p "dvm2rjq6ngk@GZN-wth"
```

**TOKEN WASTE PREVENTION RULES:**
1. NEVER try CLI commands without `-p "dvm2rjq6ngk@GZN-wth"`
2. NEVER use API/SQL editor when user says "use CLI"
3. NEVER search for these connection details - they're documented here
4. ALWAYS start with proven templates, never experiment
5. Service role password for CLI, regular password for direct connections
