# LinkedIn Ingestion Project Rules

## üß† CRITICAL MEMORY SYSTEMS - READ FIRST
**Essential files to check for previous learnings and context:**
1. **`MEMORY_KEEPER_MCP_GUIDE.md`** - Complete MCP usage with correct parameter formats  
2. **`learning/lessons-learned.md`** - Token waste prevention and critical learnings
3. **Memory Keeper MCP** - Query with `search_nodes` for `LinkedIn Ingestion` or `Supabase_CLI`
4. **`.warp.md`** (this file) - Project-specific patterns and connection details

When working on this project:

## Project Context
- This is a FastAPI project for LinkedIn data ingestion and processing
- The codebase has been recently consolidated and cleaned up (session 2025-08-07)
- All API endpoints are consolidated in `main.py` 
- All 167 tests are consolidated in `app/tests/` directory
- Use `python run_tests.py` to run the test suite
- Integrated with Supabase for database and API functionality

## Development Workflow
- Check `linkedin-ingestion-SESSION_HISTORY.md` for recent work context
- Session files are stored in `sessions/` directory
- Current work: V1.88 Prompt Templates Management System implementation
- Memory Keeper MCP should be used for all context persistence
- Hybrid AgentOS + WARP.md approach for robust session management

## Memory Management
- Use Memory Keeper MCP to store key entities and relationships
- V1.85 LLM Profile Scoring: COMPLETE (August 12th, 2025)
- Current focus: V1.88 Prompt Templates Management System
- Session hibernation should create timestamped files in sessions/ directory
- Always update SESSION_HISTORY.md for continuity between sessions
- Use Warp Terminal's built-in context through this .warp.md file

## Architecture Notes
- Removed deprecated API route files from `app/api/routes/*.py`
- Eliminated scattered test files from root directory
- Unified test execution with single runner script
- V1.85 LLM-based profile scoring: COMPLETE with OpenAI integration
- V1.88 Prompt templates system: Database-driven template management
- Production-first approach based on V1.85 lessons learned

## Testing
- All tests pass after structural consolidation
- Use unified test runner: `python run_tests.py`
- Test directory: `app/tests/`
- Total test count: 247 tests (with complete V1.85 implementation)
- Always test with virtual environment: `source venv/bin/activate`

## V1.85 LLM Profile Scoring Status: ‚úÖ COMPLETE
- ‚úÖ Task 1: Database Schema & Job Infrastructure (COMPLETE)
- ‚úÖ Task 2: OpenAI Integration & LLM Service (COMPLETE)
- ‚úÖ Task 3: API Endpoints Implementation (COMPLETE)
- ‚úÖ Task 4: Async Job Processing System (COMPLETE)
- ‚úÖ Task 5: Integration Testing & Production Deployment (COMPLETE)
- Production validated with Christopher Leslie profile scoring

## V1.88 Prompt Templates Management Status: üöß READY FOR IMPLEMENTATION
- ‚ùå Task 1: Database Schema & Migration Implementation
- ‚ùå Task 2: Pydantic Models & Data Validation
- ‚ùå Task 3: Template Service Layer Implementation
- ‚ùå Task 4: API Endpoints & Controllers
- ‚ùå Task 5: LLM Scoring Integration
- ‚ùå Task 6: Comprehensive Testing Implementation
- ‚ùå Task 7: Health Check & Monitoring Integration

## Key Context Files
- `agent-os/specs/2025-08-13-v188-prompt-templates-management/` - Current V1.88 spec for implementation
- `agent-os/specs/2025-08-11-v185-llm-profile-scoring/` - COMPLETED V1.85 LLM scoring system
- `agent-os/product/roadmap.md` - Project roadmap and current status
- `sessions/linkedin-ingestion-session-2025-08-12-225847.md` - V1.85 completion and V1.88 planning

## Production Environment
- **Domain**: smooth-mailbox-production.up.railway.app
- **API Key**: li_HieZz-IjBp0uE7d-rZkRE0qyy12r5_ZJS_FR4jMvv0I
- **Database**: Supabase with scoring_jobs table deployed
- **LLM Scoring**: Fully operational with OpenAI integration
- **Test Profile ID**: 435ccbf7-6c5e-4e2d-bdc3-052a244d7121 (Christopher Leslie)

## üö® CRITICAL: Railway Logs - PREVENT TERMINAL HANGS

**MANDATORY: NEVER use `railway logs` directly - it hangs the terminal**

**SAFE LOG VIEWING METHODS:**
1. **Use safe_logs.sh script** (prevents hangs with timeout):
   ```bash
   ./safe_logs.sh [number_of_lines]     # Default: 20 lines
   ./safe_logs.sh 50                    # Get 50 recent lines
   ./safe_logs.sh 100                   # Get 100 recent lines
   ```

2. **Use Railway web dashboard** (safest option):
   ```bash
   railway open   # Opens project dashboard in browser
   ```
   - Navigate to Logs section in the web UI
   - Use filtering and context features
   - No CLI terminal hang issues

**CRITICAL RULES:**
- ‚ùå NEVER run `railway logs` directly (causes terminal freeze)
- ‚ùå NEVER run `railway logs | head` or similar (still hangs)
- ‚úÖ ALWAYS use `./safe_logs.sh` for CLI log viewing
- ‚úÖ ALWAYS use `railway open` for full log access
- üí° The safe_logs.sh uses `gtimeout` to kill hung processes after 5 seconds

## üîç RAILWAY WEBHOOK DEBUGGING - ISSUE IDENTIFIED

**ROOT CAUSE: Railway cannot reach localhost webhook URLs from their servers**

**Webhook Detection Logic:** ‚úÖ WORKING CORRECTLY
- Tests confirmed webhook detection patterns work properly
- Successfully detects: `deployment.success`, `status: 'SUCCESS'`, `status: 'DEPLOYED'` 
- Correctly ignores: `deployment.started`, `deployment.building`, `deployment.failed`

**Core Problem:** ‚ùå NETWORK ACCESSIBILITY
- Railway servers cannot reach `http://localhost:3000` from external network
- Webhook listener works locally but Railway can't send webhooks to localhost
- Previous webhook setup attempts failed due to this fundamental issue

**IMPROVED DEPLOYMENT MONITORING SOLUTIONS:**
1. **Smart Deployment Monitor** (recommended):
   ```bash
   python3 scripts/smart_deployment_monitor.py --timeout 600 --interval 15
   ```
   - Uses health endpoint polling (no network issues)
   - Detects service restarts and version changes
   - Monitors enhanced endpoint availability
   - Much more reliable than webhook approach

2. **Webhook with ngrok tunnel** (advanced):
   ```bash
   python3 scripts/smart_deployment_monitor.py --use-webhook
   ```
   - Creates public tunnel with ngrok (if installed)
   - Provides setup instructions for Railway webhook configuration
   - Falls back to polling if tunnel fails

**CRITICAL LESSON:** Never rely on localhost webhooks for external services
- Always use polling-based monitoring for Railway deployments
- Webhooks require public endpoints or tunneling solutions
- Health endpoint polling is more reliable and simpler to implement

## üö® CRITICAL: Supabase CLI Operations - PREVENT TOKEN WASTE

**MEMORIZE: linkedin-ingestion Project Database Details**
- **Service Role Password**: `dvm2rjq6ngk@GZN-wth` (CLI operations only)
- **Project URL**: `https://nggbqpkfdbhakbkbtxgc.supabase.co`
- **Project ID**: `nggbqpkfdbhakbkbtxgc`

**MANDATORY Command Templates (ALWAYS include password):**
```bash
# Database dump - NEVER experiment, use this exact format
supabase db dump -p "dvm2rjq6ngk@GZN-wth" -s public

# Migration push - NEVER omit password
supabase db push -p "dvm2rjq6ngk@GZN-wth"

# Migration repair - ALWAYS include password
supabase migration repair [timestamp] -p "dvm2rjq6ngk@GZN-wth"
```

**TOKEN WASTE PREVENTION RULES:**
1. NEVER try CLI commands without `-p "dvm2rjq6ngk@GZN-wth"`
2. NEVER use API/SQL editor when user says "use CLI"
3. NEVER search for these connection details - they're documented here
4. ALWAYS start with proven templates, never experiment
5. Service role password for CLI, regular password for direct connections
